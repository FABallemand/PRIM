\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}
Since the beginning of computers, the amount of raw data has always exceeded the capacity of storage mediums. With the quantity of created data approaching 500 terabytes every day in 2024, data compression has become ubiquitous. Whether it is to improve storage capacity or enhance data transfer, data compression is used regardless of the scale. Let us focus on image data. Image files on a computer disk can be compressed to take less storage space. Video streams of data transfered over the internet are compressed in order to make transfers more efficient in terms of ressource availability and energy consumption. Both examples highlight the induced improved user experience (ability to store more files on the same disk and fast download speeds).

However data compression comes at a cost. There is a limit to how many bits can be removed while encoding the exact same data. Past this limit, some data is lost which can degrade the content of the file to various degrees. This process is called lossy image compression (as opposed to lossless compression) and it introduces a tradeoff between data compression and data quality. There is a wide variety of (deterministic) algorithms used to compress data. Some of them are design to focus on compression at the cost of quality. Such algorithms (like GIF) work well for simple schematics and diagrams but introduce artifacts on more complex files like photographs. State of the art algorithms (like JPEG or Webp) manage to perform data compression while preserving the majority of the file content. Some artifacts can be added but they can be negligeables depending of the use case (for instance they are often not visible to the human eye without close inspection).

With the recent progress of machine learning, new compression algorithms appeared. These probabilistic algorithms are based on neural networks that can learn a better way to encode data with a minimum number of bits. The only issue being that these methods require way more processing power than previous state of the art compression algorithms. The goal of this project is to achieve realtime image compression on resource constrained platforms. Chapter \ref{sota} summarises state of the art methods. The first results achieved in the context of this project mainly consist in reproducing state of the art results and are presented in Chapter \ref{results}.
% Section \ref{lic} explains how machine learned image compression works and Section \ref{sota} briefly presents state of the art methods. Finally, ... of this project is shown in Section \ref{results}.