\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}
Since the beginning of computers, the amount of raw data has always exceeded the capacity of storage mediums. With the quantity of created data approaching 500 terabytes every day in 2024, data compression has become ubiquitous. Whether it is to improve storage capacity or enhance data transfer, data compression is used regardless of the scale. Let us focus on image data. Image files on a computer disk can be compressed to take less storage space, so users can store more files on the same disk. Video streams of data transfered over the internet are compressed in order to make transfers more efficient in terms of network availability and energy consumption resulting in faster transfer speeds. Both examples highlight how image compression improves the user experience.

However data compression comes at a cost. There is a limit to how many bits can be removed while encoding the exact same data. Past this limit, some data is lost which can degrade the content of the file to various degrees. This process, called lossy image compression (as opposed to lossless compression), introduces a tradeoff between data compression and data quality. There is a wide variety of (deterministic) algorithms used to compress data. Some of them are designed to focus on compression at the cost of quality. Such algorithms (e.g. GIF) work well for simple schematics and diagrams but introduce artifacts on more complex files like photographs. Other algorithms focus more on visual fidelity. These algorithms (e.g. JPEG or Webp) can compress complex images without too much degradation. That is, the compressed image can be deteriorated by artifacts that are often negligeable. Most of the time, such artifacts are not visible to the human eye without close inspection, making these algorithms suitable for mainstream use.

\acrfull{lic} is located at the intersection of the \acrfull{ml} and the image processing domains. With the recent progress of \acrshort{ml}, new compression methods based on \acrfull{nn} have been developped. Leveraging tools introduced for other \acrshort{ml} computer vision tasks, these models can learn how to encode and decode images with different \acrfull{rd} tradeoffs. Recent research work present deep \acrshort{nn}s that consistently outperform the most optimised conventional algorithms. The only issue being that these results comes at the cost of a lot of processing power, making these techiques unsuitable for real-time use on resource constrained platforms thus preventing their deployment and use for mainstream applications.

Achieving real-time image compression on resource constrained platforms requires a lot more research in the fields of \acrshort{ml} and hardware specific implementation. With a scope limited to applying frugal \acrshort{ml} techniques to \acrshort{lic}, this project can be considered as a small step in the right direction to achieve fast \acrshort{nn} based image compression. Chapter \ref{lic} explains how machine learned image compression works and Chapter \ref{sota} summarises state-of-the-art models. In Chapter \ref{part_1}, we assess our ability to replicate and evaluate state-of-the-art results. Chapter \ref{part_2} focuses on knowledge distillation applied to \acrshort{lic}.
% The method, experiments, and results obtained during this project are discussed in detail.