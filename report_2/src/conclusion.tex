\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}
Data compression and especially image compression is a crucial tool in information technology. Whether it is to save storage space or to improve networks efficiency, the goal is to represent the same information with the minimum amount of bits. Lossless compression is limited, leaving place to lossy compression. This compression method introduces a tradeoff between storage space and data quality/integrity.

Using existing deep learning and computer vision tools, \acrshort{lic} aims to provide a new way to perform image compression providing better results than previous deterministic codecs. However, deep learning is often sinonymous of high processing power and \acrshort{lic} is not exempt. \acrshort{ml} based image compression relies on large \acrshort{nn}, making it unsusable on resource constrained platforms.

The final goal of this project is to explore technique allowing to use \acrshort{lic} on these low power devices possibly in real-time. Our resarch is centered on \acrshort{kd}, a frugal \acrshort{ai} training paradigm created to achieve better results with small models. The general idea is to guide the small \acrshort{nn} with another larger and more performant model during training. The small model is generally refered as the student model while the larger one is the teacher.

After having set-up our experiment environment based on the compressAI library and utilising publicly available models and datasets, we reproduce state-of-the-art \acrshort{lic} results ensuring that we are working in good conditions and fair comparison with state-of-the-art results.

Following a step-by-step approach, we first assess the effectiveness of \acrshort{kd} on image reconstruction task with well known \acrshort{ae} models and \acrshort{lic} specific architectures. Visual inspection and quantitative measures assert knowledge distilled models perform better than their non-distilled conter part. Similar results are observed when training teacher and student models for \acrshort{lic}. Focusing on \acrshort{rd} performance, results demonstrate that \acrshort{kd} students reach performance on par with their teacher. We also explore the impact of teacher on the students and remark that teacher focusing on image quality guide their students toward better image quality as opposed to bit rate focused teacher that train high compression students. We also show that a the loss function used for training students impact their \acrshort{rd} performance, opening new research opportunities. Finally, we measure the resource savings achieved by using knowledge distilled \acrshort{nn}. In our experiments with different student architectures, we discover that students use less memory, require less processing power and consume less energy. To some extend, these savings come without noticeable impact on compression performance. It is up to the user to decide what tradeoff can be made for each real world application: having better image compression or saving more resources.

Our study already introduce potential future research work regarding the influence of the teacher model on its student and various \acrshort{kd} settings such as the distillation loss. In future work, we would also like to investigate \acrhsort{kd} with other \acrhost{lic} models as more recent and complex models would open new possibilities: we could implement \acrshort{kd} on the hyper latent space of scale hyperprior and joint autoregressive models introduced by Ballé et al. \cite{ballé2018variationalimagecompressionscale, minnen2018jointautoregressivehierarchicalpriors} as well as experimenting with transformer based models \cite{zou2022devildetailswindowbasedattention}. Going back to our study's initial context where a powerful server compress an image once and broadcast it to many edge devices for decompression, we could also explore hybrid architecture with encoders larger than decoders. In this case, the encoder part could be the same as the teacher and be frozen at training time. This set-up would make distilling image compression knowledge even more relevant as the large model would still partially be used eliminating any debate regarding the necessity of training a large model beforehand. But other limitations of \acrshort{kd} are still concerning notably the additional training complexity when building a student. This process depends on many settings and hyperparameters: it generally depends on the teacher quality and does not guarantee a limited teacher/student performance gap possibly due to overfitting the teacher. Data dependency was not studied in this work but could influence the results (sizes or change in the teacher/student datasets). Finally, in a context where regulation encourage models explainability, the process of distillation is not ideal. Due to their limited internal representation and learning from the output (and only the output) of a teacher, students lack feature importance and decision-rationale making them even less transparent than standard models.