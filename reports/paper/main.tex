\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\graphicspath{{media/}}     % organize your images and other figures under media/ folder

%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{Learned Image Compression on FPGA}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}



  
%% Title
\title{Learned Image Compression on FPGA
%%%% Cite as
%%%% Update your official citation here when published 
\thanks{\textit{\underline{Citation}}: 
\textbf{Authors. Title. Pages.... DOI:000000/11111.}} 
}

\author{
  Fabien Allemand\\
  Télécom SudParis\\
  \texttt{fabien.allemand@telecom-sudparis.eu} \\
  %% examples of more authors
   \And
  Author3 \\
  Affiliation \\
  Univ \\
  City\\
  \texttt{email@email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}


\begin{document}
\maketitle


\begin{abstract}
  Learned image compression sits at the intersection of machine learning and image processing. With recent advancements in deep learning, neural network-based compression methods have emerged. In this process, a convolutional encoder maps the image to a low-dimensional latent space, which is then quantized, entropy-coded into a binary bitstream, and transmitted to the receiver. At the receiver end, the bitstream is entropy-decoded, and a convolutional decoder reconstructs an approximation of the original image. Recent research suggests that these models consistently outperform conventional codecs. However, they demand significant processing power, making them unsuitable for real-time use on resource-constrained platforms, which hinders their deployment in mainstream applications. This study aims to reduce the resource requirements of neural networks used for image compression by leveraging knowledge distillation—a training paradigm where smaller neural networks, partially trained on the outputs of larger, more complex models, can achieve better performance than when trained independently. Our work demonstrates that knowledge distillation can be effectively applied to image compression tasks: i) across various architecture sizes, ii) to achieve different image quality/bit rate tradeoffs, and iii) to save processing and energy resources. This approach introduces new settings and hyperparameters, and future research could explore the impact of different teacher models, as well as alternative loss functions. Knowledge distillation could also be extended to hyper latent spaces or transformer-based models. However, the real-world application of these models may raise concerns regarding regulatory compliance due to their reduced explainability.
\end{abstract}


% keywords can be removed
\keywords{Learned Image Compression \and Knowledge Distillation \and Frugal AI}


\section{Introduction}
Since the beginning of computers, the volume of raw data has consistently outpaced the capacity of storage mediums. In 2024, data creation is approaching 500 terabytes daily, making data compression a necessity. Compression is essential for improving storage capacity and enhancing data transfer, regardless of scale. For example, image files can be compressed on a computer's hard drive to save space, enabling users to store more files. Similarly, video streams are compressed during internet transmission to optimize network efficiency and reduce energy consumption, resulting in faster transfer speeds. In both cases, image compression significantly improves user experience.

However data compression comes at a cost. There is a limit to how many bits can be removed while encoding the exact same data. Beyond this threshold, some data is discarded, potentially degrading the content. This process, known as lossy image compression (as opposed to lossless compression), introduces a tradeoff between reducing file size and maintaining data quality. Various deterministic algorithms are employed for compression, each prioritizing different aspects. For instance, algorithms like GIF focus on maximizing compression at the cost of quality, making them suitable for simple graphics but prone to artifacts in complex images like photographs. On the other hand, algorithms like JPEG or WebP prioritize visual fidelity, compressing intricate images with minimal visible degradation, which often goes unnoticed by the human eye making these algorithms suitable for mainstream use.

Learned Image Compression (LIC) sits at the intersection of Machine Learning (ML) and image processing. Recent advancements in ML have led to the emergence of LIC, where Neural Networks (NN) are leveraged to improve compression techniques. By using tools from ML and computer vision, LIC models can learn how to encode and decode images with different Rate-Distortion (RD) tradeoffs. On the encoder side, the image is projected to a low-dimensional latent space by a convolutional encoder. This representation is quantised, entropy-coded in the form of a binary bitstream and sent to the receiver. At the receiver, the bitstream is entropy-decoded, a convolutional decoder projects such representation back to the pixel domain, recovering an approximate representation of the image. Recent research work present deep NNs that consistently outperform the most optimised conventional algorithms. But their high computational demands make them unsuitable for real-time applications on resource-constrained devices, thus preventing their deployment and use for mainstream applications.

Achieving real-time image compression on resource constrained platforms requires a lot more research in the fields of ML and hardware specific implementation. This project aims to contribute to this challenge by exploring frugal ML techniques for LIC. This project can be considered as a step in the right direction to achieve fast NN based image compression. Chapter \ref{lic} provides an overview of how machine learned image compression works and Chapter \ref{sota} reviews state-of-the-art models. In Chapter \ref{part_1}, we assess our ability to replicate and evaluate state-of-the-art results. Chapter \ref{part_2} explores the application of frugal Artificial Intelligence (AI) techniques, such as knowledge distillation, to LIC.

\section{Headings: first level}
\label{sec:headings}

\lipsum[4] See Section \ref{sec:headings}.

\subsection{Headings: second level}
\lipsum[5]
\begin{equation}
\xi _{ij}(t)=P(x_{t}=i,x_{t+1}=j|y,v,w;\theta)= {\frac {\alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}{\sum _{i=1}^{N} \sum _{j=1}^{N} \alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}}
\end{equation}

\subsubsection{Headings: third level}
\lipsum[6]

\paragraph{Paragraph}
\lipsum[7]

\section{Examples of citations, figures, tables, references}
\label{sec:others}
\lipsum[8] \cite{kour2014real,kour2014fast} and see \cite{hadash2018estimate}.

The documentation for \verb+natbib+ may be found at
\begin{center}
  \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}
Of note is the command \verb+\citet+, which produces citations
appropriate for use in inline text.  For example,
\begin{verbatim}
   \citet{hasselmo} investigated\dots
\end{verbatim}
produces
\begin{quote}
  Hasselmo, et al.\ (1995) investigated\dots
\end{quote}

\begin{center}
  \url{https://www.ctan.org/pkg/booktabs}
\end{center}


\subsection{Figures}
\lipsum[10] 
See Figure \ref{fig:fig1}. Here is how you add footnotes. \footnote{Sample of the first footnote.}
\lipsum[11] 

\begin{figure}
  \centering
  \fbox{\rule[-.5cm]{4cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Sample figure caption.}
  \label{fig:fig1}
\end{figure}

\subsection{Tables}
\lipsum[12]
See awesome Table~\ref{tab:table}.

\begin{table}
 \caption{Sample table title}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{2}{c}{Part}                   \\
    \cmidrule(r){1-2}
    Name     & Description     & Size ($\mu$m) \\
    \midrule
    Dendrite & Input terminal  & $\sim$100     \\
    Axon     & Output terminal & $\sim$10      \\
    Soma     & Cell body       & up to $10^6$  \\
    \bottomrule
  \end{tabular}
  \label{tab:table}
\end{table}

\subsection{Lists}
\begin{itemize}
\item Lorem ipsum dolor sit amet
\item consectetur adipiscing elit. 
\item Aliquam dignissim blandit est, in dictum tortor gravida eget. In ac rutrum magna.
\end{itemize}


\section{Conclusion}
Data and image compression are critical in information technology, helping save storage space and improve network efficiency by reducing the amount of data needed to represent information. While lossless compression has its limitations, lossy compression introduces a tradeoff between storage efficiency and data quality. Deep learning has opened new possibilities for image compression, but it often requires significant computational power, which can limit its use on resource-constrained devices.

This project aims to explore methods for enabling image compression with deep learning models on low-power devices, potentially in real-time. We focused on Knowledge Distillation (KD), a technique that trains smaller, more efficient models (students) with guidance from larger, high-performing models (teachers). Our experiments showed that KD enhances performance in both image reconstruction tasks and compression tasks, achieving results comparable to teacher models while significantly reducing resource consumption.

We evaluated KD with various models, demonstrating that it reduces memory usage, processing power, and energy consumption without significantly impacting compression performance. Our results highlight the importance of selecting the right tradeoff between image quality and resource efficiency, depending on the application.

Looking ahead, we plan to investigate the impact of teacher models on student performance in more detail, as well as explore KD with more complex models like hyper latent space models and transformer-based models. Additionally, we aim to explore hybrid architectures where larger encoders can be used with smaller decoders. This could offer further improvements, especially in scenarios where powerful servers compress images for broadcasting to multiple edge devices.

However, challenges remain, particularly in the complexity of the KD training process, which depends on various hyperparameters and teacher quality. Future research will also need to consider data dependencies and the transparency of models, especially in contexts where model explainability is critical. Knowledge distillation, by focusing on output alone, can result in less interpretability compared to traditional models, raising concerns in regulated environments.

\section*{Acknowledgments}
Working on a six-month research project at the Institut Polytechnique de Paris has been an enriching experience, marked by numerous technical discoveries. I would like to express my gratitude to everyone who has supported and guided me throughout the project and in the writing of this report.

First and foremost, I would like to thank my supervisors, Attilio Fiandrotti, Alaa Mazouz, and Sumanta Chaudhuri, researchers at the Institut Polytechnique de Paris, for their warm welcome, the valuable time spent together, and the knowledge they shared with me.

I would also like to extend my sincere thanks to Alaa Mazouz, who directly supervised my work and collaborated with me throughout the project. His ongoing support and detailed explanations were crucial in helping me understand the tasks entrusted to me and in acquiring new skills that will undoubtedly benefit my future endeavors.

%Bibliography
\bibliographystyle{unsrt}  
\bibliography{references}  


\end{document}
