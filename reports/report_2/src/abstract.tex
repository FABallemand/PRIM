\chapter*{Abstract}
Learned image compression sits at the intersection of machine learning and image processing. With recent advancements in deep learning, neural network-based compression methods have emerged. In this process, a convolutional encoder maps the image to a low-dimensional latent space, which is then quantized, entropy-coded into a binary bitstream, and transmitted to the receiver. At the receiver end, the bitstream is entropy-decoded, and a convolutional decoder reconstructs an approximation of the original image. Recent research suggests that these models consistently outperform conventional codecs. However, they demand significant processing power, making them unsuitable for real-time use on resource-constrained platforms, which hinders their deployment in mainstream applications. This study aims to reduce the resource requirements of neural networks used for image compression by leveraging knowledge distillationâ€”a training paradigm where smaller neural networks, partially trained on the outputs of larger, more complex models, can achieve better performance than when trained independently. Our work demonstrates that knowledge distillation can be effectively applied to image compression tasks: i) across various architecture sizes, ii) to achieve different image quality/bit rate tradeoffs, and iii) to save processing and energy resources. This approach introduces new settings and hyperparameters, and future research could explore the impact of different teacher models, as well as alternative loss functions. Knowledge distillation could also be extended to hyper latent spaces or transformer-based models. However, the real-world application of these models may raise concerns regarding regulatory compliance due to their reduced explainability.