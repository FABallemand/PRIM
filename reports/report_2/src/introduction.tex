\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}
Since the beginning of computers, the volume of raw data has consistently outpaced the capacity of storage mediums. In 2024, data creation is approaching 500 terabytes daily, making data compression a necessity. Compression is essential for improving storage capacity and enhancing data transfer, regardless of scale. For example, image files can be compressed on a computer's hard drive to save space, enabling users to store more files. Similarly, video streams are compressed during internet transmission to optimize network efficiency and reduce energy consumption, resulting in faster transfer speeds. In both cases, image compression significantly improves user experience.

However data compression comes at a cost. There is a limit to how many bits can be removed while encoding the exact same data. Beyond this threshold, some data is discarded, potentially degrading the content. This process, known as lossy image compression (as opposed to lossless compression), introduces a tradeoff between reducing file size and maintaining data quality. Various deterministic algorithms are employed for compression, each prioritizing different aspects. For instance, algorithms like GIF focus on maximizing compression at the cost of quality, making them suitable for simple graphics but prone to artifacts in complex images like photographs. On the other hand, algorithms like JPEG or WebP prioritize visual fidelity, compressing intricate images with minimal visible degradation, which often goes unnoticed by the human eye making these algorithms suitable for mainstream use.


\acrfull{lic} sits at the intersection of \acrfull{ml} and image processing. Recent advancements in \acrshort{ml} have led to the emergence of \acrshort{lic}, where \acrfull{nn} are leveraged to improve compression techniques. By using tools from \acrshort{ml} and computer vision, \acrshort{lic} models can learn how to encode and decode images with different \acrfull{rd} tradeoffs. On the encoder side, the image is projected to a low-dimensional latent space by a convolutional encoder. This representation is quantised, entropy-coded in the form of a binary bitstream and sent to the receiver. At the receiver, the bitstream is entropy-decoded, a convolutional decoder projects such representation back to the pixel domain, recovering an approximate representation of the image. Recent research work present deep \acrshort{nn}s that consistently outperform the most optimised conventional algorithms. But their high computational demands make them unsuitable for real-time applications on resource-constrained devices, thus preventing their deployment and use for mainstream applications.

Achieving real-time image compression on resource constrained platforms requires a lot more research in the fields of \acrshort{ml} and hardware specific implementation. This project aims to contribute to this challenge by exploring frugal \acrshort{ml} techniques for \acrshort{lic}. This project can be considered as a step in the right direction to achieve fast \acrshort{nn} based image compression. Chapter \ref{lic} provides an overview of how machine learned image compression works and Chapter \ref{sota} reviews state-of-the-art models. In Chapter \ref{part_1}, we assess our ability to replicate and evaluate state-of-the-art results. Chapter \ref{part_2} explores the application of frugal \acrfull{ai} techniques, such as \acrsfull{kd}, to \acrshort{lic}.